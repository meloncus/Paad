{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 15:59:42.004798: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-18 15:59:42.085337: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-06-18 15:59:42.085379: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리만 임포트\n",
    "import numpy as np  # 다차원 배열을 다루기 위한 라이브러리\n",
    "import librosa  # 음악 및 오디오 분석을 위한 라이브러리\n",
    "import os  # 운영 체제와 상호 작용하기 위한 라이브러리, 파일 경로 조작 등\n",
    "import tensorflow as tf  # 머신러닝 및 신경망을 위한 라이브러리\n",
    "import pandas as pd  # 데이터 분석 및 조작을 위한 라이브러리\n",
    "from IPython.display import clear_output  # Jupyter 노트북의 출력을 지우기 위한 함수\n",
    "import matplotlib.pyplot as plt  # 데이터 시각화를 위한 라이브러리\n",
    "\n",
    "# Jupyter 노트북에서 matplotlib의 그래프를 인라인으로 표시하도록 설정\n",
    "%matplotlib inline  \n",
    "\n",
    "import seaborn as sns  # matplotlib 기반의 고급 시각화 라이브러리\n",
    "import torch  # 딥러닝 프레임워크 중 하나\n",
    "from sklearn.model_selection import train_test_split  # 데이터를 훈련 세트와 테스트 세트로 분할하기 위한 함수\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score  # 모델 평가를 위한 성능 지표 계산 함수\n",
    "from tensorflow.keras import layers, losses  # TensorFlow의 케라스 API를 사용하여 신경망의 층과 손실 함수를 정의\n",
    "from tensorflow.keras.models import Model  # TensorFlow의 케라스 API를 사용하여 모델을 정의 및 관리\n",
    "from tensorflow.python.ops.numpy_ops import np_config  # TensorFlow에서 numpy와의 호환성을 위한 설정\n",
    "np_config.enable_numpy_behavior()  # TensorFlow에서 numpy와 유사한 동작을 활성화\n",
    "\n",
    "# 사용자 정의 유틸리티 함수 임포트\n",
    "from util import play_audio, load_audio, get_features, get_mfcc, get_lmfe, get_chroma, plot_chroma, plot_mfcc, plot_lmfe\n",
    "# 오디오 데이터를 다루기 위한 사용자 정의 함수들 (재생, 로드, 특징 추출 등)\n",
    "\n",
    "# Seaborn 스타일 설정\n",
    "sns.set(style=\"white\", palette=\"muted\")  # Seaborn 그래프의 기본 스타일 설정\n",
    "sns.set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})  # Seaborn 그래프의 세부 스타일 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "def summarize_json(data, indent=0, key_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    JSON 데이터를 요약하여 구조를 출력하는 함수\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        print(' ' * indent + '{')\n",
    "        for key, value in data.items():\n",
    "            print(' ' * (indent + 2) + f'\"{key}\": ', end='')\n",
    "            summarize_json(value, indent + 2, key_prefix=key_prefix + key + \"/\")\n",
    "        print(' ' * indent + '}')\n",
    "    elif isinstance(data, list):\n",
    "        print(' ' * indent + '[')\n",
    "        if data:\n",
    "            item_types = Counter(type(item).__name__ for item in data)\n",
    "            for item_type, count in item_types.items():\n",
    "                print(' ' * (indent + 2) + f'{item_type} x {count}')\n",
    "            for item in data[:1]:  # First item example\n",
    "                summarize_json(item, indent + 2, key_prefix=key_prefix)\n",
    "            if len(data) > 1:\n",
    "                print(' ' * (indent + 2) + '...')\n",
    "        print(' ' * indent + ']')\n",
    "    else:\n",
    "        print(' ' * indent + f'{type(data).__name__}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에 안 돌리고 prepare_dataset 을 테스트해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.prepare_dataset import get_data_paths_and_labels_from_machine as get_data_from_machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.prepare_dataset import get_from_mimii\n",
    "# \"/mnt/d/silofox/paad/anomaly-example/exploring-AAD/notebooks/data/MIMII/data_-6_db/fan/id_04/normal/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir = \"/mnt/d/silofox/paad/anomaly-example/exploring-AAD/notebooks/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_dir is initialized to /mnt/d/silofox/paad/anomaly-example/exploring-AAD/notebooks/data\n",
      "['data', 'DCASE', '2020', 'dev', 'fan', 'train']\n",
      "['data', 'DCASE', '2020', 'dev', 'fan', 'test']\n",
      "['data', 'DCASE', '2020', 'eval', 'fan', 'test']\n",
      "['data', 'DCASE', '2020', 'add', 'fan', 'train']\n",
      "['data', 'DCASE', '2021', 'dev', 'fan', 'train']\n",
      "['data', 'DCASE', '2021', 'dev', 'fan', 'test']\n",
      "['data', 'DCASE', '2021', 'eval', 'fan', 'test']\n",
      "['data', 'DCASE', '2021', 'add', 'fan', 'train']\n",
      "['data', 'DCASE', '2022', 'dev', 'fan', 'train']\n",
      "['data', 'DCASE', '2022', 'dev', 'fan', 'test']\n",
      "['data', 'DCASE', '2022', 'eval', 'fan', 'test']\n",
      "['data', 'DCASE', '2022', 'add', 'fan', 'train']\n",
      "['data', 'DCASE', '2023', 'dev', 'fan', 'train']\n",
      "['data', 'DCASE', '2023', 'dev', 'fan', 'test']\n",
      "['data', 'DCASE', '2024', 'dev', 'fan', 'train']\n",
      "['data', 'DCASE', '2024', 'dev', 'fan', 'test']\n",
      "['data', 'MIMII', 'data_-6_db', 'fan', 'id_00', 'normal']\n",
      "['data', 'MIMII', 'data_-6_db', 'fan', 'id_00', 'abnormal']\n",
      "['data', 'MIMII', 'data_-6_db', 'fan', 'id_02', 'normal']\n",
      "['data', 'MIMII', 'data_-6_db', 'fan', 'id_02', 'abnormal']\n",
      "['data', 'MIMII', 'data_-6_db', 'fan', 'id_04', 'normal']\n",
      "['data', 'MIMII', 'data_-6_db', 'fan', 'id_04', 'abnormal']\n",
      "['data', 'MIMII', 'data_-6_db', 'fan', 'id_06', 'normal']\n",
      "['data', 'MIMII', 'data_-6_db', 'fan', 'id_06', 'abnormal']\n",
      "['data', 'MIMII', 'data_0_db', 'fan', 'id_00', 'normal']\n",
      "['data', 'MIMII', 'data_0_db', 'fan', 'id_00', 'abnormal']\n",
      "['data', 'MIMII', 'data_0_db', 'fan', 'id_02', 'normal']\n",
      "['data', 'MIMII', 'data_0_db', 'fan', 'id_02', 'abnormal']\n",
      "['data', 'MIMII', 'data_0_db', 'fan', 'id_04', 'normal']\n",
      "['data', 'MIMII', 'data_0_db', 'fan', 'id_04', 'abnormal']\n",
      "['data', 'MIMII', 'data_0_db', 'fan', 'id_06', 'normal']\n",
      "['data', 'MIMII', 'data_0_db', 'fan', 'id_06', 'abnormal']\n",
      "['data', 'MIMII', 'data_6_db', 'fan', 'id_00', 'normal']\n",
      "['data', 'MIMII', 'data_6_db', 'fan', 'id_00', 'abnormal']\n",
      "['data', 'MIMII', 'data_6_db', 'fan', 'id_02', 'normal']\n",
      "['data', 'MIMII', 'data_6_db', 'fan', 'id_02', 'abnormal']\n",
      "['data', 'MIMII', 'data_6_db', 'fan', 'id_04', 'normal']\n",
      "['data', 'MIMII', 'data_6_db', 'fan', 'id_04', 'abnormal']\n",
      "['data', 'MIMII', 'data_6_db', 'fan', 'id_06', 'normal']\n",
      "['data', 'MIMII', 'data_6_db', 'fan', 'id_06', 'abnormal']\n"
     ]
    }
   ],
   "source": [
    "fan_data_path, fan_data_label = get_data_from_machine(\"fan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print([].append([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_fan_data = fan_data_path[\"MIMII\"][\"data_-6_db\"]\n",
    "specific_fan_data_list = list(specific_fan_data.values())[0] + list(specific_fan_data.values())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1016\n",
      "361\n"
     ]
    }
   ],
   "source": [
    "print(len(list(specific_fan_data.values())[0]))\n",
    "print(len(list(specific_fan_data.values())[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1377"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(specific_fan_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/mnt/d/silofox/paad/anomaly-example/exploring-AAD/notebooks/data/MIMII/data_-6_db/fan/id_06/normal/00000305.wav',\n",
       " '/mnt/d/silofox/paad/anomaly-example/exploring-AAD/notebooks/data/MIMII/data_-6_db/fan/id_06/normal/00000611.wav',\n",
       " '/mnt/d/silofox/paad/anomaly-example/exploring-AAD/notebooks/data/MIMII/data_-6_db/fan/id_06/normal/00000000.wav',\n",
       " '/mnt/d/silofox/paad/anomaly-example/exploring-AAD/notebooks/data/MIMII/data_-6_db/fan/id_06/normal/00000001.wav',\n",
       " '/mnt/d/silofox/paad/anomaly-example/exploring-AAD/notebooks/data/MIMII/data_-6_db/fan/id_06/normal/00000002.wav',\n",
       " '/mnt/d/silofox/paad/anomaly-example/exploring-AAD/notebooks/data/MIMII/data_-6_db/fan/id_06/normal/00000003.wav',\n",
       " '/mnt/d/silofox/paad/anomaly-example/exploring-AAD/notebooks/data/MIMII/data_-6_db/fan/id_06/normal/00000004.wav',\n",
       " '/mnt/d/silofox/paad/anomaly-example/exploring-AAD/notebooks/data/MIMII/data_-6_db/fan/id_06/normal/00000005.wav',\n",
       " '/mnt/d/silofox/paad/anomaly-example/exploring-AAD/notebooks/data/MIMII/data_-6_db/fan/id_06/normal/00000006.wav',\n",
       " '/mnt/d/silofox/paad/anomaly-example/exploring-AAD/notebooks/data/MIMII/data_-6_db/fan/id_06/normal/00000007.wav']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specific_fan_data_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/mnt/d/silofox/paad/anomaly-example/exploring-AAD/notebooks/data/MIMII/data_-6_db/fan/id_06/abnormal/00000305.wav',\n",
       " '/mnt/d/silofox/paad/anomaly-example/exploring-AAD/notebooks/data/MIMII/data_-6_db/fan/id_06/abnormal/00000323.wav',\n",
       " '/mnt/d/silofox/paad/anomaly-example/exploring-AAD/notebooks/data/MIMII/data_-6_db/fan/id_06/abnormal/00000341.wav',\n",
       " '/mnt/d/silofox/paad/anomaly-example/exploring-AAD/notebooks/data/MIMII/data_-6_db/fan/id_06/abnormal/00000072.wav',\n",
       " '/mnt/d/silofox/paad/anomaly-example/exploring-AAD/notebooks/data/MIMII/data_-6_db/fan/id_06/abnormal/00000073.wav',\n",
       " '/mnt/d/silofox/paad/anomaly-example/exploring-AAD/notebooks/data/MIMII/data_-6_db/fan/id_06/abnormal/00000074.wav',\n",
       " '/mnt/d/silofox/paad/anomaly-example/exploring-AAD/notebooks/data/MIMII/data_-6_db/fan/id_06/abnormal/00000075.wav',\n",
       " '/mnt/d/silofox/paad/anomaly-example/exploring-AAD/notebooks/data/MIMII/data_-6_db/fan/id_06/abnormal/00000076.wav',\n",
       " '/mnt/d/silofox/paad/anomaly-example/exploring-AAD/notebooks/data/MIMII/data_-6_db/fan/id_06/abnormal/00000077.wav',\n",
       " '/mnt/d/silofox/paad/anomaly-example/exploring-AAD/notebooks/data/MIMII/data_-6_db/fan/id_06/abnormal/00000078.wav']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specific_fan_data_list[1100:1110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2050613669.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    df_fan = pd.DataFrame({\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "df_fan = pd.DataFrame({\"filename\" : fan_data_path[\"MIMII\"][\"data_-6_db\"][\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
